{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime/thoi gian chay -> change runtime type/ thay doi loai thoi gian chay -> GPU\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,  Flatten, Embedding, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect toi gg drive, tao foler chua dataset, theo duong dan /content/drive/MyDrive/foler chua dataset/SQli.csv\n",
    "df = pd.read_csv('/content/drive/MyDrive/SQLi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "data = df.values\n",
    "train_data, test_data = train_test_split(data, test_size=0.4, random_state=42)\n",
    "\n",
    "# get sentences and labels\n",
    "train_sentences = train_data[:, 0]\n",
    "train_labels = train_data[:, 1].astype(np.float64)\n",
    "\n",
    "test_sentences = test_data[:, 0]\n",
    "test_labels = test_data[:, 1].astype(np.float64)\n",
    "\n",
    "#model word2vec\n",
    "word2vec_model = Word2Vec(vector_size=300, window=3, min_count=20,negative=20,sample=6e-5, workers=4)\n",
    "\n",
    "# Build a vocabulary\n",
    "word2vec_train = [sentence.split() for sentence in train_sentences]\n",
    "phraser = Phrases(word2vec_train, min_count=20, progress_per=10000)\n",
    "biagram = Phraser(phraser)\n",
    "word2vec_train = biagram[word2vec_train]\n",
    "word2vec_model.build_vocab(word2vec_train, progress_per=10000)\n",
    "\n",
    "# train word2vec model\n",
    "word2vec_model.train(train_sentences, total_examples=word2vec_model.corpus_count, epochs=10, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save va load model tai su dung neu khong muon train lai, nho thay doi duong dan ve gg drive: /content/drive/MyDrive/j do/word2vec.model\n",
    "#save model\n",
    "word2vec_model.save('word2vec.model')\n",
    "\n",
    "# load pretrained model\n",
    "word2vec_model = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prepare embedding matrix\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "\n",
    "# tokenize\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "#add dimension in train_padded\n",
    "train_padded = np.expand_dims(train_padded, axis=2)\n",
    "test_padded = np.expand_dims(test_padded, axis=2)\n",
    "# create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word2vec_model.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)\n",
    "\n",
    "# create embedding layer\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(16, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(32, 4, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khong can thuc thi neu ko muon truc quan ma chi danh gia qua metrics\n",
    "# save tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "   pickle.dump(tokenizer, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#predict the model\n",
    "def predict(sentence):\n",
    "    sequences = tokenizer.texts_to_sequences([sentence])\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    if model.predict(padded)[0][0] > 0.5:\n",
    "        print('SQLi')\n",
    "    else:\n",
    "        print('Normal')\n",
    "\n",
    "# predict the model\n",
    "predict('SELECT Employees.LastName, COUNT ( Orders.OrderID )  AS NumberOfOrders FROM  ( Orders INNER JOIN Employees ON Orders.EmployeeID  =  Employees.EmployeeID )  GROUP BY LastName HAVING COUNT ( Orders.OrderID )  > 10;')\n",
    "predict('SELECT * FROM users WHERE username = \"admin\" AND password = \"password\" OR 1=1')\n",
    "predict('Hello World!')\n",
    "predict('union select version(),user(),3,4,--+-')\n",
    "predict('from users where id  =  1<@<@ union select 1,version()-- 1')\n",
    "predict('SELECT min (failed) FROM nation SELECT SUM(economy)')\n",
    "predict('UnIOn sElecT 1,2,3,id(),--+-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check weights and accuracy\n",
    "print(model.evaluate(test_padded, test_labels))\n",
    "\n",
    "# apply metrics on test data\n",
    "y_pred = model.predict(test_padded)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy: \",accuracy_score(test_labels, y_pred))\n",
    "\n",
    "# precision\n",
    "print(\"Precision: \",precision_score(test_labels, y_pred))\n",
    "\n",
    "# recall\n",
    "print(\"Recall: \",recall_score(test_labels, y_pred))\n",
    "\n",
    "# f1 score\n",
    "print(\"F1 score: \",f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(test_labels, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
